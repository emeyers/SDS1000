---
title: "Class 26 notes and code"
format: pdf
editor: source
editor_options: 
  chunk_output_type: console
---




 
 


# Part 1: Inference on simple linear regression using randomization methods


We can run inference on the slope of a simple linear regression model using
randomization methods. We will use the `states_smoking.rda` dataset that contains
data on cigarette consumption and lung cancer rates for each of the 50 states in
the US.




$\\$






### Part 1.1: Randomization hypothesis test for the slope in simple linear regression


We can also use randomization methods to run a hypothesis test for the slope of the
linear regression model. Let's test the hypotheses:

- Null hypothesis: There is no relationship between cigarette consumption and
lung cancer rate (slope = 0)

- Alternative hypothesis: There is a positive relationship between cigarette consumption
and lung cancer rate (slope > 0)


In symbols: 

- $H_0: \beta = 0$
- $H_A: \beta > 0$


```{r lung_cancer_hypothesis_test}

library(SDS1000)

# load the data
load("states_smoking.rda")

cigs_per_capita <- smoking$CIG
cancer_rate <- smoking$LUNG



# Step 2: get the observed slope


# fit the linear model again
lung_lm <- lm(cancer_rate ~ cigs_per_capita)

the_coefs <- coef(lung_lm)
obs_slope <- the_coefs[2]
obs_slope



# Step 3: create null distribution
null_dist <- do_it(10000) * {
  
  shuffled_cancer <- shuffle(cancer_rate)
  
  null_lm <- lm(shuffled_cancer ~ cigs_per_capita)
  
  the_coefs <- coef(null_lm)
  
  the_coefs[2]  # return the slope
  
}


# visualize the null distribution
hist(null_dist,
     main = "Null Distribution of the Slope",
     xlab = "Slope",
     xlim = c(-0.006, 0.006))

abline(v = obs_slope, col = "red")



# Step 4: calculate the p-value
p_value <- mean(null_dist >= obs_slope)
p_value


```



# Step 5: Make decision

Since the p-value is very small, we reject the null hypothesis and conclude
that there is a significant positive relationship between cigarette consumption
and lung cancer rate.



$\\$






# Part 2: Parametric confidence intervals for the slope in simple linear regression


We can also use parametric methods to create confidence intervals for the slope
in simple linear regression. To do this we can use the standard error of the
slope and the t-distribution to create a confidence interval.

The formula for the confidence interval is:
$$b \pm t^* \cdot SE_{b}$$

Where:
- $b$ is the slope of the regression line
- $t^*$ is the critical value from the t-distribution with $n - 2$ degrees of freedom
- $SE_{b}$ is the standard error of the slope


The formula for the standard error of the slope is:

$$SE_{b} = \frac{\sigma_e}{\sqrt{\sum{(x_i - \bar{x})^2}}}$$

Where:
- $\sigma_e$ is the standard deviation of the residuals
- $x_i$ are the values of the explanatory variable
- $\bar{x}$ is the mean of the explanatory variable


We will mainly rely on built in functions in R to do these calculations for us. 





$\\$





### Part 2.1: Parametric confidence interval for the slope in simple linear regression



We can extract this information from the `summary()` of the linear model object.

Alternatively, we can use the `confint()` function to get the confidence interval 
directly.



```{r lung_cancer_ci_parametric}


# fit our model again
lung_lm <- lm(cancer_rate ~ cigs_per_capita)


# summarize the model
the_summary <- summary(lung_lm)
the_summary


# get the slope
the_coefs <- coef(lung_lm)
slope <- the_coefs[2]
slope


# get the standard error of the slope
the_SEs <- the_summary$coefficients[, "Std. Error"]
SE_slope <- the_SEs[2]
SE_slope


# get the critical value for 95% confidence interval
n <- length(cigs_per_capita)
df <- n - 2
critical_value <- mosaic::ct(0.95, df)  # two-tailed, so use


# calculate the margin of error
ME <- critical_value * SE_slope
ME


# calculate the confidence interval
ci_lower <- slope - ME
ci_upper <- slope + ME
ci <- c(ci_lower, ci_upper)
ci


# Alternatively, we can use the confint() function to get the confidence interval
confint_result <- confint(lung_lm, level = 0.95)
confint_result


```


The 95% parametric confidence interval for the slope is approximately (0.0036, 0.007).






$\\$






### Part 2.2: Parametric hypothesis test for the slope in simple linear regression


We can also use parametric methods to run a hypothesis test for the slope of the
linear regression model. We can use the t-statistic and the t-distribution to
calculate the p-value.


```{r lung_cancer_hypothesis_test_parametric}

# fit our model again
lung_lm <- lm(cancer_rate ~ cigs_per_capita)


# summarize the model
the_summary <- summary(lung_lm)


# get the slope
the_coefs <- coef(lung_lm)
slope <- the_coefs[2]


# get the standard error of the slope
the_SEs <- the_summary$coefficients[, "Std. Error"]
SE_slope <- the_SEs[2]


# calculate the t-statistic
t_stat <- (slope - 0) / SE_slope  # null hypothesis slope


# get the degrees of freedom
n <- length(cigs_per_capita)
df <- n - 2


# calculate the p-value
p_value <- pt(t_stat, df, lower.tail = FALSE)  # one-tailed
p_value


```



$\\$






# Part 3: Multiple linear regression 


In multiple linear regression, we have more than one explanatory variable. The
model can be written as:
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon$$

Where:
- $y$ is the response variable
- $x_1, x_2, ..., x_p$ are the explanatory variables
- $\beta_0$ is the intercept
- $\beta_1, \beta_2, ..., \beta_p$ are the coefficients
- $\epsilon$ is the error term


We can fit multiple linear regression models in R using the `lm()` function,
just like we did for simple linear regression. We just need to include multiple
explanatory variables in the formula.


Let's use the `FirstYearGPA.rda` dataset that contains data on first year college
students. We will try to predict their GPA from the following variables: 

- High school GPA (HSGPA)
- SAT verbal scores (SATV)
- Number of humanities credits (HU)



```{r multiple_linear_regression}

# load the data
load("FirstYearGPA.rda")


# fit the multiple linear regression model
multi_lm <- lm(GPA ~ HSGPA + SATV + HU, data = FirstYearGPA)

# summarize the model
summary(multi_lm)

```


The summary output will show us the coefficients for each explanatory variable,
along with their standard errors, t-values, and p-values. We can use this
information to assess the significance of each variable in the model.


To learn more about multiple regression, take additional statistics classes (e.g.
S&DS 2300: Data Exploration and Analysis)!





