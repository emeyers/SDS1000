---
title: "Class 25 notes and code"
format: pdf
editor: source
editor_options: 
  chunk_output_type: console
---




 
 
# Part 1: Warm-up: Practice with one-way analysis of variance (ANOVA)


To see how much of a difference time of day made on the speed at which he could
download files, a college sophomore performed an experiment:

- He placed a file on a remote server and then proceeded to download it at three 
different time periods of the day (7AM, 5PM, 12AM)

- He downloaded the file 48 times in all, 16 times at each time of day, and 
recorded the time in seconds that the download took


Let's run an ANOVA hypothesis test to see if download time differs by time of day.



$\\$



Step 1: State the null and alternative hypotheses



Words:

* Null hypothesis: The mean download time is the same for all three time periods

* Alternative hypothesis: The mean download time is different for at least one 
of the time periods


Symbols:  

* $H_0: \mu_{7AM} = \mu_{5PM} = \mu_{12AM}$   
* $H_A$: At least one of the means is different  




$\\$




```{r sudoku_viz}

library(SDS1000)


# load the data
download_data <- read.csv("downloading.csv")

download_times <- download_data$Time_Sec
time_of_day <- download_data$Time_of_Day





# Check the ANOVA conditions - let's use the mosaic favstats() function 
fav_stats <- mosaic::favstats(download_times ~ time_of_day)
fav_stats

# roughly equal spread?
max(fav_stats$sd)/min(fav_stats$sd)   # less than 2, equal variance condition holds






# Step 2: visualize the data and calculate the F-statistic

boxplot(download_times ~ time_of_day,
        xlab = "Time of Day",
        ylab = "Download Time (seconds)",
        main = "Download Time by Time of Day")



# calculate the observed statistic

obs_stat <- get_F_stat(download_times, time_of_day)
obs_stat




# Step 3: Visualize the null distribution

df1 <- length(unique(time_of_day)) - 1  # K - 1
df2 <- length(download_times) - length(unique(time_of_day))  # N - K


# plot the null distribution
x_vals <- seq(0, 5, length.out = 1000)
y_vals <- df(x_vals, df1, df2)

plot(x_vals, y_vals, type = 'l')


# add a red vertical line at the observed statistic
abline(v = obs_stat, col = "red")


# Step 4: Calculate the p-value


p_value <- pf(obs_stat, df1, df2, lower.tail = FALSE)
p_value

```




$\\$


Step 5: Make decision

Since the p-value is very small, we reject the null hypothesis and conclude
that there is a significant difference in download times for at least one of the
time periods.



$\\$



Let's also look at the ANOVA table


```{r anova_table}

anova_result <- aov(download_times ~ time_of_day)
summary(anova_result)

```





$\\$




# Part 2: Inference on simple linear regression using randomization methods


We can run inference on the slope of a simple linear regression model using
randomization methods. We will use the `states_smoking.rda` dataset that contains
data on cigarette consumption and lung cancer rates for each of the 50 states in
the US.



$\\$




### Part 2.1: Bootstrap confidence interval for the slope in simple linear regression



Let's create confidence interval for the slope of a simple linear regression
model that predicts the lung cancer rate as a function of the number of
cigarettes smoked per capita.




```{r lung_cancer_ci}

# load the data
load("states_smoking.rda")

cigs_per_capita <- smoking$CIG
cancer_rate <- smoking$LUNG


# visualize the data 
plot(cigs_per_capita, cancer_rate,
     xlab = "Cigarettes per Capita",
     ylab = "Lung Cancer Rate per 100,000",
     main = "Lung Cancer Rate vs. Cigarette Consumption")



# fit the linear model
lung_lm <- lm(cancer_rate ~ cigs_per_capita)

# get the slope b
the_coefs <- coef(lung_lm)
slope <- the_coefs[2]
slope



boot_dist <- do_it(10000) * {
  
  resampled_data <- resample_pairs(cigs_per_capita, cancer_rate)
  
  resampled_cigs <- resampled_data$vector1
  resampled_cancer <- resampled_data$vector2
  
  resampled_lm <- lm(resampled_cancer ~ resampled_cigs)
  
  # visualize the bootstrap regression line
  abline(resampled_lm, col = "red")
  
  the_coefs <- coef(resampled_lm)
  
  the_coefs[2]  # return the slope
  
}



# visualize the bootstrap distribution
hist(boot_dist,
     main = "Bootstrap Distribution of the Slope",
     xlab = "Slope")

# calculate the 95% confidence interval using the percentile method
boot_SE <- sd(boot_dist)
boot_SE

critical_value <- mosaic::cnorm(0.95, side = "upper")  # for 95% confidence interval

ME <- critical_value * boot_SE

ci_lower <- slope - ME
ci_upper <- slope + ME
ci <- c(ci_lower, ci_upper)
ci


```

The 95% confidence interval for the slope is approximately (0.003, 0.007). This 
means that for an additional 1,000 cigarette smoked per capita, the predicted 
lung cancer rate increases by between 3 and 7 cases per 100,000 people.





$\\$





### Part 2.2: Randomization hypothesis test for the slope in simple linear regression


We can also use randomization methods to run a hypothesis test for the slope of the
linear regression model. Let's test the hypotheses:

- Null hypothesis: There is no relationship between cigarette consumption and
lung cancer rate (slope = 0)

- Alternative hypothesis: There is a positive relationship between cigarette consumption
and lung cancer rate (slope > 0)


In symbols: 

- $H_0: \beta = 0$
- $H_A: \beta > 0$


```{r lung_cancer_hypothesis_test}


# Step 2: get the observed slope


# fit the linear model again
lung_lm <- lm(cancer_rate ~ cigs_per_capita)

the_coefs <- coef(lung_lm)
obs_slope <- the_coefs[2]
obs_slope



# Step 3: create null distribution
null_dist <- do_it(10000) * {
  
  shuffled_cancer <- shuffle(cancer_rate)
  
  null_lm <- lm(shuffled_cancer ~ cigs_per_capita)
  
  the_coefs <- coef(null_lm)
  
  the_coefs[2]  # return the slope
  
}


# visualize the null distribution
hist(null_dist,
     main = "Null Distribution of the Slope",
     xlab = "Slope",
     xlim = c(-0.006, 0.006))

abline(v = obs_slope, col = "red")



# Step 4: calculate the p-value
p_value <- mean(null_dist >= obs_slope)
p_value


```



# Step 5: Make decision

Since the p-value is very small, we reject the null hypothesis and conclude
that there is a significant positive relationship between cigarette consumption
and lung cancer rate.



$\\$





# Part 3: Parametric confidence intervals for the slope in simple linear regression


We can also use parametric methods to create confidence intervals for the slope
in simple linear regression. To do this we can use the standard error of the
slope and the t-distribution to create a confidence interval.

The formula for the confidence interval is:
$$b \pm t^* \cdot SE_{b}$$

Where:
- $b$ is the slope of the regression line
- $t^*$ is the critical value from the t-distribution with $n - 2$ degrees of freedom
- $SE_{b}$ is the standard error of the slope


The formula for the standard error of the slope is:

$$SE_{b} = \frac{\sigma_e}{\sqrt{\sum{(x_i - \bar{x})^2}}}$$

Where:
- $\sigma_e$ is the standard deviation of the residuals
- $x_i$ are the values of the explanatory variable
- $\bar{x}$ is the mean of the explanatory variable


We will mainly rely on built in functions in R to do these calculations for us. 



$\\$



### Part 3.1: Parametric confidence interval for the slope in simple linear regression



We can extract this information from the `summary()` of the linear model object.

Alternatively, we can use the `confint()` function to get the confidence interval 
directly.



```{r lung_cancer_ci_parametric}


# fit our model again
lung_lm <- lm(cancer_rate ~ cigs_per_capita)


# summarize the model
the_summary <- summary(lung_lm)
the_summary


# get the slope
the_coefs <- coef(lung_lm)
slope <- the_coefs[2]
slope


# get the standard error of the slope
the_SEs <- the_summary$coefficients[, "Std. Error"]
SE_slope <- the_SEs[2]
SE_slope


# get the critical value for 95% confidence interval
n <- length(cigs_per_capita)
df <- n - 2
critical_value <- mosaic::ct(0.95, df)  # two-tailed, so use


# calculate the margin of error
ME <- critical_value * SE_slope
ME


# calculate the confidence interval
ci_lower <- slope - ME
ci_upper <- slope + ME
ci <- c(ci_lower, ci_upper)
ci


# Alternatively, we can use the confint() function to get the confidence interval
confint_result <- confint(lung_lm, level = 0.95)
confint_result


```


The 95% parametric confidence interval for the slope is approximately (0.0036, 0.007).




$\\$






### Part 3.2: Parametric hypothesis test for the slope in simple linear regression


We can also use parametric methods to run a hypothesis test for the slope of the
linear regression model. We can use the t-statistic and the t-distribution to
calculate the p-value.


```{r lung_cancer_hypothesis_test_parametric}

# fit our model again
lung_lm <- lm(cancer_rate ~ cigs_per_capita)


# summarize the model
the_summary <- summary(lung_lm)


# get the slope
the_coefs <- coef(lung_lm)
slope <- the_coefs[2]


# get the standard error of the slope
the_SEs <- the_summary$coefficients[, "Std. Error"]
SE_slope <- the_SEs[2]


# calculate the t-statistic
t_stat <- (slope - 0) / SE_slope  # null hypothesis slope


# get the degrees of freedom
n <- length(cigs_per_capita)
df <- n - 2


# calculate the p-value
p_value <- pt(t_stat, df, lower.tail = FALSE)  # one-tailed
p_value


```







