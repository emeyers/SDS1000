---
title: "Practice Session 11"
format: pdf
editor: source
editor_options: 
  chunk_output_type: console
---

# Part 1 Review of Inference for (Simple) Linear Regression

As we saw last week, hypothesis testing can be done for more than sample means and proportions. We can also test hypotheses relating to regression parameters, like $\beta_1$, the slope, and $\beta_0$, the y-intercept. The hypotheses typically test whether the parameter is greater than zero, less than zero, or not equal to zero. 

# Question 1

Do larger countries have more rural land? Investigate this question using the `TenCountries` data set from the `Lock5Data` library. Fit a regression model, and run a hypothesis test to see if countries with more area tend to have a higher percentage of rural land. In other words, run a test to see if the slope coefficient is greater than zero. **Assume that the percent of rural land is the outcome for the model, and the area of the country is the predictor.**

a) First, create a scatterplot to visualize the relationship between `Area` and `PctRural`

```{r}
# your code here
```

b) State the null and alternative hypotheses using symbols



c) Fit the regression model with `PctRural` as the outcome, and `Area` as the predictor. Extract the slope coefficient by using the `coef()` function, and selecting the second value (e.g., `coef(my_model)[2]`).


```{r}
# your code here
```


d) Create a null distribution by using the `do_it` function. The approach you will want to take is to fit a regression model inside the `do_it` call (maybe call it `curr_model`), and you will use `PctRural` as the outcome, and a shuffled `Area` as the predictor. You can shuffle the `Area` variable using the `shuffle()` function. Extract the slope coefficient after fitting each model.

```{r}
# your code here
```


e) Plot a histogram of your null distribution, and add a red vertical line at the observed slope.

```{r}
# your code here
```



f) Calculate the p-value by seeing the proportion of null values that are more extreme than the one you observed.

```{r}
# your code here
```


g) State your conclusion.


h) Compare your final result to the output from using the `summary()` function on your regression model. What do you notice?

```{r}
# your code here
```


i) A collaborator raises concerns about this analysis. She notes that there is a disagreement between your permutation results, and the output from the `summary()` function. What could be causing this issue? Could there be an issue with the data set that you used?

# Question 2

Are heavier fish longer? Data collected from fish markets provides measurements on the weight and height of different species of fish. Fit a regression model and run a hypothesis test to see if heavier fish tend to also be longer. **Assume that fish height is the outcome for the model, and fish weight is the predictor.**

The data is available in the `Fish.csv` data set. Use `read.csv()` to load the data.

a) First, create a scatterplot to visualize the relationship between `Height` and `Weight`.


```{r}
# your code here
```


b) State the null and alternative hypotheses using symbols


c) Fit the regression model with `Height` as the outcome, and `Weight` as the predictor. Extract the slope coefficient by using the `coef()` function, and selecting the second value (e.g., `coef(my_model)[2]`).


```{r}
# your code here
```



d) Create a null distribution by using the `do_it` function. The approach you will want to take is to fit a regression model inside the `do_it` call (maybe call it `curr_model`), and you will use `Height` as the outcome, and a shuffled `Weight` as the predictor. You can shuffle the `Weight` variable using the `shuffle()` function. Extract the slope coefficient after fitting each model.

```{r}
# your code here
```


e) Plot a histogram of your null distribution, and add a red vertical line at the observed slope


```{r}
# your code here
```



f) Calculate the p-value by seeing the proportion of null values that are more extreme than the one you observed.

```{r}
# your code here
```


g) State your conclusion.


h) Compare your final result to the output from using the `summary()` function on your regression model. What do you notice?


```{r}
# your code here
```


i) The same collaborator raises concerns about the fish analysis. She notes that the scatterplot doesn't appear to be linear. Are your regression results still valid? Can you think of any solutions for fixing the data so that we can run a linear regression?

```{r}
# your code here
```



# Part 2 Inference for (Multiple) Linear Regression

So far, we have fit regression models where we had a single predictor or variable relating to our outcome variable. However, we often are interested in understanding the relationship among the outcome and two or more other variables. Regression with more than one predictor is known as multiple linear regression. 


# Part 2: Multiple regression

# Question 3


Predicting `Armspan`  from both `Height` and `Foot`  (Footlength)  a sample of high school students in `PASenior`. How well do they work together in a multiple regression model to predict Armspan?


```{r}
library(SDS1000)
library(Lock5Data)
data(PASeniors)

cPASeniors<- na.omit(PASeniors)
```


1.) Fit a simple linear regression model to predict `Armspan` from `Height`.

```{r}
# your code here
```



2.) Fit a simple linear regression model to predict `Armspan` foot length in  `Foot`.


```{r}
# your code here
```




3.) Compare the two models in terms of : the significance of the predictors, the standard deviation of the residuals and the Coefficient of determination $R^2$.




a) Find the slope for each model. Which predictor has a larger slope?


b) Find the standard deviation of the error term for each model. Which predictor has a smaller standard deviation of the error term?


c) Find the percentage of variability in arm span explained by each predictor. Which predictor explains more variability?


d) Based on parts a)â€“c), which variable is more effective for predicting arm span?



4) Now fit a multiple linear regression to predict `ArmSpan` from the two predictors: foot length in  `Foot` and `Height`.

```{r}
# your code here
```


a) What arm span would the fitted model predict for a student who is 180 cm tall and has a foot that is 26 cm long?

b) Are both Height and Foot useful in the multiple linear regression  model for Armspan? Justify your answer.

c) How much of the variability in Armspan do the two predictors together explain?


**Answers**

```{r}
library(SDS1000)
library(Lock5Data)
data(PASeniors)

cPASeniors<- na.omit(PASeniors)
```


1.) Fit a simple linear regression model to predict `ArmSpan` from `Height`.

```{r}

# fit a simple linear regression to predict `Armspan` from ` Height`


summary(lm(Armspan ~ Height, data = cPASeniors))


```



2.) Fit a simple linear regression model to predict `Armspan` foot length in  `Foot`.

```{r}

# fit a simple linear regression to predict `Armspan` from ` Foot`


summary(lm(Armspan ~  Foot, data = cPASeniors))

```


3.) Compare the two models in terms of : the significance of the predictors, the standard deviation of the residuals and the Coefficient of determination $R^2$.



a) Foot ( $b1$= 3.4835) has a larger slope than Height ($b1= 0.91491$).

b) Height ( $se =9.414$) has a smaller standard deviation of error than Foot ($se = 9.937$).

c) Height with $R^2=51.5 \%$ explains more variability in arms span than Foot with
$R^2=46.0 \%$.  

d) Both the smaller standard deviation of error in (b) and the larger $R^2$. In (c) indicate that Height is somewhat more effective than Foot for predicting Armspan. The larger slope for Foot is not so relevant since it also has a much larger standard error for the slope.



4) Now fit a multiple linear regression to predict `ArmSpan` from the two predictors: foot length in  `Foot` and `Height`.



```{r}
# your code here
```


 
a) The fitted model is Armspan $=8.52+0.7356 \cdot$ Height $+1.4477 \cdot$ Foot. For a student with Height $=180$ and Foot $=26$ the predicted arm span is



b) The p-values for the individual t-tests for both predictors are essentially zero, so we have strong evidence that both Height and Foot are effective in this model to predict Armspan.


c) The output shows $R^2=61.75 \%$, so the model based on Height and Foot explains $61.75 \%$ of the variability of the Armspan measurements for these students.
